{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a80792-599c-4b8e-a91c-83cf3885632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\n",
    "    \"explodinggradients/amnesty_qa\",\n",
    "    \"english_v3\",\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c91e1b4-f627-48b7-a02b-fcb9b6110ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88091d41-44f6-4b06-ae9b-a6b1bfeb7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset[\"eval\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d275583-8ec5-42b2-b02d-61352164a4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_input': 'What are the global implications of the USA Supreme Court ruling on abortion?',\n",
       " 'reference': \"The global implications of the USA Supreme Court ruling on abortion are significant. The ruling has led to limited or no access to abortion for one in three women and girls of reproductive age in states where abortion access is restricted. These states also have weaker maternal health support, higher maternal death rates, and higher child poverty rates. Additionally, the ruling has had an impact beyond national borders due to the USA's geopolitical and cultural influence globally. Organizations and activists worldwide are concerned that the ruling may inspire anti-abortion legislative and policy attacks in other countries. The ruling has also hindered progressive law reform and the implementation of abortion guidelines in certain African countries. Furthermore, the ruling has created a chilling effect in international policy spaces, empowering anti-abortion actors to undermine human rights protections.\",\n",
       " 'response': \"The global implications of the USA Supreme Court ruling on abortion can be significant, as it sets a precedent for other countries and influences the global discourse on reproductive rights. Here are some potential implications:\\n\\n1. Influence on other countries: The Supreme Court's ruling can serve as a reference point for other countries grappling with their own abortion laws. It can provide legal arguments and reasoning that advocates for reproductive rights can use to challenge restrictive abortion laws in their respective jurisdictions.\\n\\n2. Strengthening of global reproductive rights movements: A favorable ruling by the Supreme Court can energize and empower reproductive rights movements worldwide. It can serve as a rallying point for activists and organizations advocating for women's rights, leading to increased mobilization and advocacy efforts globally.\\n\\n3. Counteracting anti-abortion movements: Conversely, a ruling that restricts abortion rights can embolden anti-abortion movements globally. It can provide legitimacy to their arguments and encourage similar restrictive measures in other countries, potentially leading to a rollback of existing reproductive rights.\\n\\n4. Impact on international aid and policies: The Supreme Court's ruling can influence international aid and policies related to reproductive health. It can shape the priorities and funding decisions of donor countries and organizations, potentially leading to increased support for reproductive rights initiatives or conversely, restrictions on funding for abortion-related services.\\n\\n5. Shaping international human rights standards: The ruling can contribute to the development of international human rights standards regarding reproductive rights. It can influence the interpretation and application of existing human rights treaties and conventions, potentially strengthening the recognition of reproductive rights as fundamental human rights globally.\\n\\n6. Global health implications: The Supreme Court's ruling can have implications for global health outcomes, particularly in countries with restrictive abortion laws. It can impact the availability and accessibility of safe and legal abortion services, potentially leading to an increase in unsafe abortions and related health complications.\\n\\nIt is important to note that the specific implications will depend on the nature of the Supreme Court ruling and the subsequent actions taken by governments, activists, and organizations both within and outside the United States.\",\n",
       " 'retrieved_contexts': [\"- In 2022, the USA Supreme Court handed down a decision ruling that overturned 50 years of jurisprudence recognizing a constitutional right to abortion.\\n- This decision has had a massive impact: one in three women and girls of reproductive age now live in states where abortion access is either totally or near-totally inaccessible.\\n- The states with the most restrictive abortion laws have the weakest maternal health support, higher maternal death rates, and higher child poverty rates.\\n- The USA Supreme Court ruling has also had impacts beyond national borders due to the geopolitical and cultural influence wielded by the USA globally and the aid it funds.\\n- SRR organizations and activists across the world have expressed fear about the ruling laying the groundwork for anti-abortion legislative and policy attacks in other countries.\\n- Advocates have also observed the ruling's impact on progressive law reform and the stalling of the adoption and enforcement of abortion guidelines in certain African countries.\\n- The ruling has created a chilling effect in international policy spaces, emboldening anti-abortion state and non-state actors to undermine human rights protections.\",\n",
       "  'The USA Supreme Court ruling on abortion has sparked intense debates and discussions not only within the country but also around the world. Many countries look to the United States as a leader in legal and social issues, so the decision could potentially influence the policies and attitudes towards abortion in other nations.',\n",
       "  \"The ruling may also impact international organizations and non-governmental groups that work on reproductive rights and women's health issues. Depending on the outcome, there could be shifts in funding, advocacy efforts, and collaborations with American counterparts, leading to ripple effects in the global fight for reproductive justice.\"]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"eval\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad969d72-7e72-4342-a10b-878259eb170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, SemanticSimilarity\n",
    "from ragas import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d6c140c-f18d-4541-9ee0-efde4e56242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List, Optional\n",
    "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
    "import google.generativeai as genai\n",
    "\n",
    "from langchain_core.language_models.llms import LLM\n",
    "\n",
    "class GeminiLLM(LLM):\n",
    "    \"\"\"custom model pakai gemini\"\"\"\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        \n",
    "        llm = genai.GenerativeModel(\n",
    "            model_name=\"tunedModels/gemini-welllahh-zerotemp-lrfv-3536\", \n",
    "        )  # buat jawab pertanyaan medis\n",
    "\n",
    "        ans = (\n",
    "            llm.generate_content(prompt, generation_config={\"temperature\": 0.2})\n",
    "            .candidates[0]\n",
    "            .content.parts[0]\n",
    "            .text\n",
    "        )\n",
    "        return ans\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model. Used for logging purposes only.\"\"\"\n",
    "        return \"gemini-welllahh-zerotemp-lrfv-3536\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b5d4711-3d30-4cb4-af17-528451634929",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = GeminiLLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2a78481-e42e-4ef9-90aa-62190b3d90fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faktor risiko kanker paru-paru meliputi:\n",
      "\n",
      "* Merokok: Merokok adalah faktor risiko kanker paru-paru yang paling umum. Merokok dapat menyebabkan kanker paru-paru, dan orang yang merokok lebih banyak dan lebih lama memiliki risiko lebih tinggi untuk terkena kanker paru-paru.\n",
      "* Paparan asap rokok: Orang yang terpapar asap rokok yang dihembuskan oleh perokok memiliki risiko lebih tinggi untuk terkena kanker paru-paru.\n",
      "* Paparan radon: Radon adalah gas radioaktif yang dapat menyebabkan kanker paru-paru. Radon dapat ditemukan di rumah dan bangunan lainnya.\n",
      "* Paparan asbes: Asbes adalah mineral yang dapat menyebabkan kanker paru-paru. Asbes dapat ditemukan di beberapa tempat kerja, seperti tambang dan pabrik.\n",
      "* Paparan bahan kimia lainnya: Beberapa bahan kimia lainnya dapat menyebabkan kanker paru-paru, seperti arsenik, kromium, dan nikel.\n",
      "* Riwayat keluarga kanker paru-paru: Orang yang memiliki anggota keluarga yang terkena kanker paru-paru memiliki risiko lebih tinggi untuk terkena kanker paru-paru.\n",
      "* Usia: Risiko kanker paru-paru meningkat seiring bertambahnya usia.\n",
      "* Jenis kelamin: Pria lebih berisiko terkena kanker paru-paru daripada wanita.\n",
      "* Ras: Orang kulit hitam lebih berisiko terkena kanker paru-paru daripada orang kulit putih.\n",
      "* Riwayat penyakit paru-paru: Orang yang memiliki riwayat penyakit paru-paru, seperti penyakit paru obstruktif kronis (PPOK) dan penyakit paru interstisial, memiliki risiko lebih tinggi untuk terkena kanker paru-paru.\n",
      "* Riwayat kanker lainnya: Orang yang memiliki riwayat kanker lainnya, seperti kanker kepala dan leher, kanker lambung, dan kanker kandung kemih, memiliki risiko lebih tinggi untuk terkena kanker paru-paru.\n",
      "* Paparan radiasi: Orang yang terpapar radiasi, seperti dari sinar-X atau terapi radiasi, memiliki risiko lebih tinggi untuk terkena kanker paru-paru.\n",
      "* Paparan polusi udara: Orang yang terpapar polusi udara, seperti dari lalu lintas atau pabrik, memiliki risiko lebih tinggi untuk terkena kanker paru-paru.\n",
      "* Paparan asap dari kebakaran hutan: Orang yang terpapar asap dari kebakaran hutan memiliki risiko lebih tinggi untuk terkena kanker paru-paru.\n",
      "\n",
      "Faktor-faktor ini dapat meningkatkan risiko seseorang untuk terkena kanker paru-paru, tetapi tidak semua orang yang memiliki faktor risiko ini akan terkena kanker paru-paru.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gemini.invoke(\"apa faktor resiko penyakit kanker paru-paru?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfc821-8f7b-4fd0-9af7-d114ccf0519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
    "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.runnables import run_in_executor\n",
    "\n",
    "\n",
    "class CustomChatModelAdvanced(BaseChatModel):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    \"\"\"The name of the model\"\"\"\n",
    "   \n",
    "    def _generate(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \"\"\"Override the _generate method to implement the chat model logic.\n",
    "\n",
    "        This can be a call to an API, a call to a local model, or any other\n",
    "        implementation that generates a response to the input prompt.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        # Replace this with actual logic to generate a response from a list\n",
    "        # of messages.\n",
    "        last_message = messages[-1]\n",
    "        tokens = gemini.invoke(last_message)\n",
    "        message = AIMessage(\n",
    "            content=tokens,\n",
    "            additional_kwargs={},  # Used to add additional payload (e.g., function calling request)\n",
    "\n",
    "        )\n",
    "        ##\n",
    "\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation])\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"gemini-welllahh-zerotemp-lrfv-3536\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9a589df-0cd6-4590-852c-b844811a532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GOOGLE_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Provide your Google API key here\")\n",
    "os.environ[\"GEMINI_API_KEY\"] =  os.environ[\"GOOGLE_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a292e188-c24e-4cad-b52a-2fcd84f41ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "\n",
    "embed = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0aa57f32-5b70-4ba6-8138-4acaf2e236e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "evaluator_llm = LangchainLLMWrapper(gemini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9078b0fe-62bd-4f58-b3d0-1c30d78a12e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LangchainLLMWrapper(run_config=RunConfig(timeout=180, max_retries=10, max_wait=60, max_workers=16, exception_types=(<class 'Exception'>,), log_tenacity=False, seed=42), multiple_completion_supported=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8d18f6ef-1d8b-4d7a-af5c-41703a23d80c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     LLMContextRecall(llm\u001b[38;5;241m=\u001b[39mevaluator_llm), \n\u001b[1;32m      3\u001b[0m     FactualCorrectness(llm\u001b[38;5;241m=\u001b[39mevaluator_llm), \n\u001b[1;32m      4\u001b[0m     Faithfulness(llm\u001b[38;5;241m=\u001b[39mevaluator_llm),\n\u001b[1;32m      5\u001b[0m     SemanticSimilarity(embeddings\u001b[38;5;241m=\u001b[39mevaluator_embeddings)\n\u001b[1;32m      6\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ragas/_analytics.py:130\u001b[0m, in \u001b[0;36mtrack_was_completed.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m t\u001b[38;5;241m.\u001b[39mAny:\n\u001b[1;32m    129\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m--> 130\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     track(IsCompleteEvent(event_type\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, is_completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ragas/evaluation.py:202\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(metric, MetricWithLLM) \u001b[38;5;129;01mand\u001b[39;00m metric\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 202\u001b[0m         llm \u001b[38;5;241m=\u001b[39m \u001b[43mllm_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     metric\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m llm\n\u001b[1;32m    204\u001b[0m     llm_changed\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/ragas/llms/base.py:375\u001b[0m, in \u001b[0;36mllm_factory\u001b[0;34m(model, run_config, default_headers, base_url)\u001b[0m\n\u001b[1;32m    372\u001b[0m     default_headers \u001b[38;5;241m=\u001b[39m helicone_config\u001b[38;5;241m.\u001b[39mdefault_headers()\n\u001b[1;32m    373\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m helicone_config\u001b[38;5;241m.\u001b[39mbase_url\n\u001b[0;32m--> 375\u001b[0m openai_model \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LangchainLLMWrapper(openai_model, run_config)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:551\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient(proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy)\n\u001b[1;32m    550\u001b[0m     sync_specific \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_client}\n\u001b[0;32m--> 551\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.0/lib/python3.11/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "metrics = [\n",
    "    LLMContextRecall(llm=evaluator_llm), \n",
    "    FactualCorrectness(llm=evaluator_llm), \n",
    "    Faithfulness(llm=evaluator_llm),\n",
    "    SemanticSimilarity(embeddings=evaluator_embeddings)\n",
    "]\n",
    "results = evaluate(dataset=eval_dataset, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b363a56-becd-4135-8ed2-c0b02d1dd8b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba705d7-8075-4dde-91cc-40398c97a15c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (OS)",
   "language": "python",
   "name": "os_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
